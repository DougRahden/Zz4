# === Cell 1: Install necessary packages ===
!pip install pandas openpyxl requests python-Levenshtein

# === Cell 2: Imports and logging ===
import pandas as pd
import requests
import time
import logging
from difflib import SequenceMatcher
from Levenshtein import ratio as levenshtein_ratio

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# === Cell 3: Configure API access ===
API_KEY = 'your-api-key-here'
INSTITUTION_TOKEN = 'your-institutional-token-here'

headers = {
    'X-ELS-APIKey': API_KEY,
    'X-ELS-Insttoken': INSTITUTION_TOKEN,
    'Accept': 'application/json'
}

AUTHOR_SEARCH_URL = "https://api.elsevier.com/content/search/author"
SCOPUS_SEARCH_URL = "https://api.elsevier.com/content/search/scopus"

# === Cell 4: Test SCOPUS API Key and Token ===
test_params = {
    'query': 'AUTHLASTNAME(Einstein) AND AUTHFIRST(Albert)',
    'count': 1
}

test_resp = requests.get(AUTHOR_SEARCH_URL, headers=headers, params=test_params)
print("Test Status Code:", test_resp.status_code)

# Show a short snippet of the response content (optional)
try:
    print("Test Response Snippet:", test_resp.json())
except Exception as e:
    print("Could not parse JSON. Raw text:", test_resp.text[:300])

# === Cell 5: Load Excel File ===
input_path = 'D:/aa030/input/aa-counterintel-input.xlsx'
df = pd.read_excel(input_path, engine='openpyxl')
print(f"Loaded {len(df)} authors from input file.")

# === Cell 6a: Define helper for fuzzy affiliation match ===
def fuzzy_match(a, b):
    a, b = a.lower(), b.lower()
    return levenshtein_ratio(a, b) > 0.80 or a in b or b in a

# === Cell 6b: Document-first disambiguation using SCOPUS metadata ===

from collections import defaultdict
from datetime import datetime

primary_output = []
secondary_output = []
summary_log = []
api_calls = {'doc_search': 0, 'doc_retrieve': 0}

max_docs_per_name = 100
max_docs_per_page = 25
fuzzy_name_threshold = 0.85

def name_fuzzy_match(first_target, first_candidate):
    return levenshtein_ratio(first_target.lower(), first_candidate.lower()) >= fuzzy_name_threshold or \
           first_target.lower().startswith(first_candidate.lower()) or \
           first_candidate.lower().startswith(first_target.lower())

def search_documents_by_name(first, last):
    base_url = "https://api.elsevier.com/content/search/scopus"
    query = f"AUTHLASTNAME({last}) AND AUTHFIRST({first})"
    params = {
        'query': query,
        'count': max_docs_per_page,
        'start': 0,
        'sort': 'coverDate:desc',
        'view': 'COMPLETE'
    }
    docs = []
    while len(docs) < max_docs_per_name:
        resp = requests.get(base_url, headers=headers, params=params)
        api_calls['doc_search'] += 1
        if resp.status_code != 200:
            logging.warning(f"[API] Failed doc search for {first} {last}, status: {resp.status_code}")
            break
        entries = resp.json().get('search-results', {}).get('entry', [])
        if not entries:
            break
        docs.extend(entries)
        params['start'] += max_docs_per_page
        if len(entries) < max_docs_per_page:
            break
        time.sleep(1)
    return docs

def extract_publication_year(entry):
    date = entry.get('prism:coverDate')
    if not date:
        return None
    try:
        return int(date[:4])
    except:
        return None

for idx, row in df.iterrows():
    first = str(row.get('first_name', '')).strip()
    middle = str(row.get('middle_name', '')).strip()
    last = str(row.get('last_name', '')).strip()
    if not first or not last:
        continue
    full_first = f"{first} {middle}".strip()
    excel_name = f"{full_first} {last}".strip()

    known_affils = []
    for i in range(1, 5):
        aff = str(row.get(f'affiliation_{i}', '')).strip()
        year = row.get(f'affiliation_{i}_year')
        if aff and pd.notna(year):
            known_affils.append((aff.lower(), int(year)))

    docs = search_documents_by_name(full_first, last)
    author_scores = defaultdict(lambda: {'score': 0, 'name': '', 'matched_docs': []})

    for doc in docs:
        pub_year = extract_publication_year(doc)
        eid = doc.get('eid')
        title = doc.get('dc:title', 'Untitled')
        authors = doc.get('author', [])
        if not isinstance(authors, list):
            continue

        for author in authors:
            candidate_id = author.get('authid', 'N/A')
            candidate_name = author.get('authname', 'Unknown')
            parts = candidate_name.split(',')
            if len(parts) != 2:
                continue
            candidate_last = parts[0].strip()
            candidate_first = parts[1].strip()
            if candidate_last.lower() != last.lower():
                continue
            if not name_fuzzy_match(first, candidate_first):
                continue

            affils = author.get('affiliation', [])
            if isinstance(affils, dict):
                affils = [affils]

            for aff in affils:
                aff_name = aff.get('affilname', '').lower()
                aff_city = aff.get('affiliation-city', '')
                aff_country = aff.get('affiliation-country', '')
                for known_affil, known_year in known_affils:
                    if fuzzy_match(known_affil, aff_name):
                        if pub_year is None or abs(pub_year - known_year) <= 6:
                            author_scores[candidate_id]['score'] += 1
                            author_scores[candidate_id]['name'] = f"{candidate_first} {candidate_last}"
                            author_scores[candidate_id]['matched_docs'].append({
                                'title': title,
                                'eid': eid,
                                'year': pub_year,
                                'affiliation': f"{aff_name}, {aff_city}, {aff_country}"
                            })

    if not author_scores:
        logging.warning(f"[Internal] No matched SCOPUS ID for {excel_name}")
        summary_log.append({
            'input_name': excel_name,
            'matched_scopus_id': None,
            'score': 0,
            'docs_scanned': len(docs)
        })
        continue

    best_id, best_data = max(author_scores.items(), key=lambda x: x[1]['score'])
    summary_log.append({
        'input_name': excel_name,
        'matched_scopus_id': best_id,
        'score': best_data['score'],
        'docs_scanned': len(docs)
    })

    for d in best_data['matched_docs']:
        primary_output.append([best_data['name'], best_id, d['title'], d['affiliation'], d['year']])

        for author in doc.get('author', []):
            if author.get('authid') == best_id:
                continue
            co_name = author.get('authname', 'Unknown')
            co_id = author.get('authid', 'N/A')
            co_affils = author.get('affiliation', [])
            if isinstance(co_affils, dict):
                co_affils = [co_affils]
            affil_parts = []
            for aff in co_affils:
                parts = [aff.get('affilname'), aff.get('affiliation-city'), aff.get('affiliation-country')]
                affil_parts.append(", ".join([p for p in parts if p]))
            affil_str = " / ".join(affil_parts) if affil_parts else "N/A"
            secondary_output.append([co_name, co_id, best_data['name'], d['title'], affil_str])

# === Print Summary ===
print("\n=== SCOPUS API CALL STATS ===")
print(f"Document search calls: {api_calls['doc_search']}")

print("\n=== MATCH SUMMARY ===")
for entry in summary_log:
    print(f"Input: {entry['input_name']}")
    print(f"  Matched SCOPUS ID: {entry['matched_scopus_id'] or 'None'}")
    print(f"  Score: {entry['score']} | Docs scanned: {entry['docs_scanned']}")
    print("---")


# === Cell 7a: Process Primary Author Output ===
    for pub in matched_pubs:
        title = pub.get('dc:title', 'Untitled')
        year = pub.get('prism:coverDate', '')[:4]
        affil_str = "Affiliation Unknown"
        for auth in pub.get('author', []):
            if auth.get('authid') == matched_author[0]:
                affils = auth.get('affiliation', [])
                if isinstance(affils, dict): affils = [affils]
                affil_parts = []
                for aff in affils:
                    parts = [aff.get('affilname'), aff.get('affiliation-city'), aff.get('affiliation-country')]
                    affil_parts.append(", ".join([p for p in parts if p]))
                affil_str = " / ".join(affil_parts)
                break
        primary_output.append([matched_author[1], title, affil_str])

    # === Cell 7b: Process Co-authors ===
    for pub in matched_pubs:
        title = pub.get('dc:title', 'Untitled')
        for auth in pub.get('author', []):
            if auth.get('authid') == matched_author[0]:
                continue
            name = auth.get('authname', 'Name Unknown')
            affils = auth.get('affiliation', [])
            if isinstance(affils, dict): affils = [affils]
            affil_parts = []
            for aff in affils:
                parts = [aff.get('affilname'), aff.get('affiliation-city'), aff.get('affiliation-country')]
                affil_parts.append(", ".join([p for p in parts if p]))
            affil_str = " / ".join(affil_parts) if affil_parts else "Affiliation Unknown"
            secondary_output.append([name, matched_author[1], title, affil_str])

# === Cell 8: Save Output Files ===
output_dir = 'D:/aa030/output/'
pd.DataFrame(primary_output, columns=["Primary Author", "Paper Title", "Affiliation"]).to_csv(output_dir + 'primary_output.csv', index=False)
pd.DataFrame(secondary_output, columns=["Coauthor", "Primary Author", "Paper Title", "Affiliation"]).to_csv(output_dir + 'secondary_output.csv', index=False)

print("Files saved.")
