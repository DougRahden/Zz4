# === Cell 1: Install necessary packages ===
!pip install pandas openpyxl requests python-Levenshtein

# === Cell 2: Imports and logging ===
import pandas as pd
import requests
import time
import logging
from difflib import SequenceMatcher
from Levenshtein import ratio as levenshtein_ratio

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# === Cell 3: Configure API access ===
API_KEY = 'your-api-key-here'
INSTITUTION_TOKEN = 'your-institutional-token-here'

headers = {
    'X-ELS-APIKey': API_KEY,
    'X-ELS-Insttoken': INSTITUTION_TOKEN,
    'Accept': 'application/json'
}

AUTHOR_SEARCH_URL = "https://api.elsevier.com/content/search/author"
SCOPUS_SEARCH_URL = "https://api.elsevier.com/content/search/scopus"

# === Cell 4: Test SCOPUS API Key and Token ===
test_params = {
    'query': 'AUTHLASTNAME(Einstein) AND AUTHFIRST(Albert)',
    'count': 1
}

test_resp = requests.get(AUTHOR_SEARCH_URL, headers=headers, params=test_params)
print("Test Status Code:", test_resp.status_code)

# Show a short snippet of the response content (optional)
try:
    print("Test Response Snippet:", test_resp.json())
except Exception as e:
    print("Could not parse JSON. Raw text:", test_resp.text[:300])

# === Cell 5: Load Excel File ===
input_path = 'D:/aa030/input/aa-counterintel-input.xlsx'
df = pd.read_excel(input_path, engine='openpyxl')
print(f"Loaded {len(df)} authors from input file.")

# === Cell 6a: Define helper for fuzzy affiliation match ===
def fuzzy_match(a, b):
    a, b = a.lower(), b.lower()
    return levenshtein_ratio(a, b) > 0.80 or a in b or b in a

# === Cell 6b: Prioritize exact match first, then fallback to fuzzy with affiliation priority ===

from collections import defaultdict

primary_output = []
secondary_output = []
summary_log = []
api_calls = {'author_search': 0, 'doc_search': 0, 'abstract_retrieval': 0}

max_docs_per_author = 100
fuzzy_affil_threshold = 0.9
ignore_tokens = ["university", "college", "institute", "laboratory", "school", "of", "the", "and", "&", "center"]

def clean_for_matching(text):
    words = [w for w in text.lower().split() if w not in ignore_tokens]
    return " ".join(words)

def fuzzy_match_cleaned(a, b):
    a_clean = clean_for_matching(a)
    b_clean = clean_for_matching(b)
    return levenshtein_ratio(a_clean, b_clean) >= fuzzy_affil_threshold or a_clean in b_clean or b_clean in a_clean

def search_scopus_authors(first, last):
    query = f"AUTHLASTNAME({last}) AND AUTHFIRST({first})"
    url = "https://api.elsevier.com/content/search/author"
    params = {'query': query, 'count': 25, 'start': 0}
    authors = []
    print(f"[API] Searching authors for: {first} {last}")
    while True:
        print(f"  → Page start={params['start']}")
        resp = requests.get(url, headers=headers, params=params)
        api_calls['author_search'] += 1
        if resp.status_code != 200:
            print(f"  [ERROR] Status {resp.status_code}")
            break
        entries = resp.json().get('search-results', {}).get('entry', [])
        for entry in entries:
            scopus_id = entry.get('dc:identifier', '').split(':')[-1]
            name_data = entry.get('preferred-name', {})
            full_name = f"{name_data.get('given-name', '')} {name_data.get('surname', '')}".strip()
            raw_affil = entry.get('affiliation-current', {}).get('affiliation-name', '')
            current_affil = raw_affil.strip() if raw_affil else 'N/A'
            authors.append((scopus_id, full_name, current_affil))
        if len(entries) < 25:
            break
        params['start'] += 25
        time.sleep(1)
    return authors

def pull_documents_by_auid(scopus_id):
    url = "https://api.elsevier.com/content/search/scopus"
    docs = []
    params = {
        'query': f"AU-ID({scopus_id})",
        'count': 25,
        'start': 0,
        'view': 'COMPLETE',
        'sort': 'coverDate:desc'
    }
    while len(docs) < max_docs_per_author:
        print(f"[API] Pulling docs for SCOPUS ID {scopus_id}, start={params['start']}")
        resp = requests.get(url, headers=headers, params=params)
        api_calls['doc_search'] += 1
        if resp.status_code != 200:
            print(f"  [ERROR] Status {resp.status_code}")
            break
        entries = resp.json().get('search-results', {}).get('entry', [])
        print(f"  → Retrieved {len(entries)} docs this page")
        if not entries:
            break
        for doc in entries:
            if not doc.get('eid') or not doc.get('dc:title'):
                print(f"  [SKIP] Missing eid or title, skipping doc")
                continue
            docs.append(doc)
        params['start'] += 25
        if len(entries) < 25:
            print(f"  → Page returned less than 25 entries. Stopping pagination.")
            break
        time.sleep(1)
    print(f"  → Total documents collected: {len(docs)}")
    return docs

def get_authors_from_eid(eid):
    url = f"https://api.elsevier.com/content/abstract/eid/{eid}"
    resp = requests.get(url, headers=headers)
    api_calls['abstract_retrieval'] += 1
    if resp.status_code != 200:
        return []
    try:
        data = resp.json()['abstracts-retrieval-response']
        authors = data['authors']['author']
        affils = data.get('affiliation', [])
        if isinstance(affils, dict):
            affils = [affils]
        affil_lookup = {a.get('@id'): a for a in affils}
        return [
            {
                'name': f"{a.get('ce:given-name', '')} {a.get('ce:surname', '')}".strip(),
                'id': a.get('@auid', 'N/A'),
                'affiliation': affil_lookup.get(
                    a.get('affiliation', {}).get('@id'), {}).get('affilname', 'N/A')
            } for a in authors
        ]
    except:
        return []

for idx, row in df.iterrows():
    first = str(row.get('first_name', '')).strip()
    last = str(row.get('last_name', '')).strip()
    if not first or not last:
        continue
    excel_name = f"{first} {last}"
    print(f"\n[START] Processing {excel_name}")

    known_affils = []
    for i in range(1, 5):
        aff = str(row.get(f'affiliation_{i}', '')).strip()
        if aff:
            known_affils.append((aff.lower(), i))  # store priority index

    candidates = search_scopus_authors(first, last)
    best_match = None
    best_priority = float('inf')
    for scopus_id, full_name, current_affil in candidates:
        print(f"  → Candidate: {full_name} (ID: {scopus_id}) | Affil: {current_affil}")
        for affil, priority in known_affils:
            if current_affil != 'N/A' and current_affil.lower() == affil:
                best_match = (scopus_id, full_name, current_affil)
                best_priority = -1  # exact match wins
                print(f"    [EXACT MATCH] '{current_affil}' == '{affil}'")
                break
            elif current_affil != 'N/A' and fuzzy_match_cleaned(current_affil, affil):
                if best_priority > priority:
                    best_match = (scopus_id, full_name, current_affil)
                    best_priority = priority
                    print(f"    [FUZZY MATCH] Fuzzy match to '{affil}' (priority {priority})")
                break

    if not best_match:
        summary_log.append({'input': excel_name, 'scopus_id': None, 'matched_affil': None, 'docs': 0})
        print(f"  [MISS] No SCOPUS ID match for {excel_name}")
        continue

    scopus_id, matched_name, matched_affil = best_match
    print(f"  [SELECTED] {matched_name} | SCOPUS ID: {scopus_id} | Affil: {matched_affil or 'N/A'}")

    docs = pull_documents_by_auid(scopus_id)
    summary_log.append({'input': excel_name, 'scopus_id': scopus_id, 'matched_affil': matched_affil, 'docs': len(docs)})

    for doc in docs:
        title = doc.get('dc:title', 'Untitled')
        year = doc.get('prism:coverDate', 'Unknown')
        eid = doc.get('eid')
        primary_output.append([matched_name, scopus_id, title, matched_affil or 'N/A', year])

        coauthors = get_authors_from_eid(eid)
        for co in coauthors:
            if co['id'] == scopus_id:
                continue
            secondary_output.append([co['name'], co['id'], matched_name, title, co['affiliation']])

# === Summary ===
print("\n=== API CALL STATS ===")
for k, v in api_calls.items():
    print(f"{k}: {v}")

print("\n=== SUMMARY ===")
for entry in summary_log:
    print(f"Input: {entry['input']} | SCOPUS ID: {entry['scopus_id'] or 'None'} | Affil: {entry['matched_affil'] or 'N/A'} | Docs: {entry['docs']}")

# === Cell 7: Save final outputs to CSV ===

import pandas as pd

# Define headers based on output content
primary_cols = ["Author Name", "SCOPUS ID", "Paper Title", "Affiliation", "Publication Year"]
secondary_cols = ["Coauthor Name", "Coauthor SCOPUS ID", "Primary Author", "Paper Title", "Coauthor Affiliation"]

# Save to CSV
pd.DataFrame(primary_output, columns=primary_cols).to_csv("primary_output.csv", index=False)
pd.DataFrame(secondary_output, columns=secondary_cols).to_csv("secondary_output.csv", index=False)

print("[SAVED] primary_output.csv →", len(primary_output), "rows")
print("[SAVED] secondary_output.csv →", len(secondary_output), "rows")
